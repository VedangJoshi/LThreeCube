===========================================================================================================================================

* Assignment 1: Making sense out of HTTP log file
 
* Solution:
	1. input file : 
	2. src: readlog.py 
	3. output: default

* Features and Functionalities: 
	1. This program generates output.txt file that contains HTTP Log Info in a proper human-readable format as a basic output
	2. ADVANCED : 
	(Installation steps follow)
	
	a. We thought of using a database to store all the entries as it would be much more feasible and efficient to store data of more than one 
	   weblog file and perform all the aggregation functions over this complete dataset. Also, this eliminates the burden on the main memory 
	   for large dataset.
	b. MongoDB has been used as the backend owing to its flexible collection formats (HTTP Dataset does not follow fix format in all headers) 
	   and MapReduce functionality. This hugely increases the speed and scalability of the data storage and analysis.
	   We have used Java Servlet API to retrieve data from this MongoDB and displayed it in tabular form and pie charts. 
	   The fact that MongoDB has been used makes it easy to add any kind of filtering and aggregation queries as and when required.
	c. Also, using Java Servlet facilitates use of unending web-technology resources like D3, JQuery, Bootstrap, AngularJS, etc. easily 
	   for aesthetic visual representation of data.

* Additional Requirements and Installation Steps:	
	1. Install MongoDB :
			> Run "mongo_install.bash" (*Note : Run it only if MongoDB is not present on your system, else data loss might 
			  occur!)
			OR
			> Follow steps on "http://docs.mongodb.org/manual/"
			
	2. Please make appropriate changes in source code of Analytics/src/java/newpackage/Analytics.java (database and 
	   collection fields of Mongo Connection).
	3. Deploy the Analytics.war on any Java Server and run the application.   

* Flow of Program:
	1. Pass weblog.txt as a command line argument to 'readlog.py'
	2. REGEX has been used to parse each HTTP Header based on the standard header format
	3. This 'readlog.py' creates a new database named 'log' in MongoDB dynamically and stores all the HTTP Data in a collection 
	   named 'http' - SCREENSHOT ATTACHED
	   (*Note : Use "> sudo service mongod start" command to start mongod server before executing this step )
	4. Also a file 'output.txt' is generated that contains all the HTTP Log Data parsed
	5. Now that all the data has been stored in MongoDB, J2EE is used to retrieve this data and render it to any Java Servlet where it can 
	   be viewed in required manner.

===========================================================================================================================================

* Assignment 2: Is Birthday paradox really valid?

* Solution:
	1. input 		: none (Verifies for 1 to 366 persons on its own, default 10000 runs for high accuracy)
	2. src 			: bdayparadox_verified.c 
	3. output 		: stdout
	4. requirements : no additional installations required

* Flow of Program:
	1. Calculate probability by running the Birthday Paradox Random Experiment n no. of times - will give test results. (n = 10000 default)
	2. Calculate using simple probability formulae (mathematically)
	3. Print both results in tabular form to compare them and prove

* Logic:
	1. Generating random b'day between day 1 to 366 and assigning to people
	2. Marking 1 for each repetition occured in respective array index
	3. Run steps 1, 2 for persons = 1 to 366 (as 366 persons gives 100% chance of repetition is known)
	4. Run steps 1, 2, 3 for 10000 times and calculate probability 
	5. Calculate probability for the same sample space using simple probability formulae (mathematically)
	6. Present in tabular form side-by-side for easy comparison

===========================================================================================================================================

* Assignment 3 : TCP/IP data 

* Solution:
	1. input 		: 1st parameter = PCAP/CAP test file ( runs for all 4 : arp-storm.pcap,tcp-ecn-sample.pcap,http_witp_jpegs.cap,bgp.pcap )
					  2nd parameter = output file name for info generated
	2. src 			: PCAP_parser.c ( packetread.c is included as header )
	3. output 		: DETAILED INFORMATION of all packets
					  SUMMARY at the end of the output file
					  {argv[2]}.txt is the output file that contains all these DETAILED FIELDS of the packets and SUMMARY
	4. requirements : sudo apt-get install libpcap-dev ( for C library that supports pcap - network traffic capture interface )

* Flow of Program:
	1. Open the .pcap file i.e. get a pointer to that file
	2. Iterate over and read all the packets in the file
	3. Print all the data in the packet in HEX form in the IC file, timestamp of the file follows at the end
	4. Read the IC file and convert the HEX data in appropriate form. Simple packet format used for finding out the fields 
	5. Print all this data in output file
	6. DETAILED SUMMARY for the complete PCAP file follows

===========================================================================================================================================

* Assignment 4 : Simple Version Control

* Solution:
	1. input 		: initial commit file that has to be created and saved elsewhere is passed as argv[1]
	2. src 			: svc.c
	3. output 		: menu-driven version control system
	4. requirements : no additional installations required

* Flow of Program:
	1. Write an initial commit (say test.txt) and save it before executing svc.c
	2. Run svc.c, pass test.txt as an argument
	3. Select option from menu, like:
	   a. Display contents of the latest version
	   b. Commit (1-delete a line, 2-append a new line at the end)
	   c. Revert to Nth version
	   d. Get latest version number
	   e. Exit 

* Features and Functionalities: 
	1. Lets the user close the program and still access all the previously created versions on next execution
	2. We have created a group of every 5 successive versions and stored it in external file
	   This saves an overhead of opening every previous version file of the version we wish to revert to by using Hashing
	3. Dynamic hash tables are created as a map for knowing contents of different versions efficiently (named .maphashtab)
	4. All the intermediate versions are stored as hidden files (.filename) in the same directory 	
	5. To keep track of the latest version and number of lines in the initial commit file, external file (.bitmap) is used

===========================================================================================================================================

* Assignment 5 : Write a program to list duplicate files from hard drive

* Solution:
	1. input 		: choice of directory to run the code on (auto-menu driven)
	2. src 			: delete_duplicates.cpp
	3. output 		: i) list of all duplicate files in the given directory path and sud-directories
					  ii) an option to delete specific files is given to user
	4. requirements : no additional installations required

* Flow of Program:
	1. Select the directory to search for duplicates
	2. Display all the duplicates sequentially (pre-order recursive traversal used)
	3. If you wish to delete one or more duplicate files, select the index
	4. Repeat steps 1, 2, 3 till user wishes to exit

* Logic:
	1. Pass the directory to search for duplicates to function 'nftw(3)'
	2. nftw(3) is present in 'ftw.h'. It walks through the directory tree that is located under the directory dirpath, and calls fn()
	3. Duplicate finding logic is used in the fn() passed to nftw()
   	4. Duplicate file names are stored in a map where Filename is the key and Vector of strings is the value
	   Vector contains addresses of duplicate files
	5. If user wants to delete any of these files, take the required input, traverse over the vector and delete these files
	6. Repeat steps 1-5 till user wishes to exit

===========================================================================================================================================
